from fastapi import FastAPI, Request, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
from openai import OpenAI
from prompt_template import make_prompt
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import faiss, numpy as np, csv, os, time
import pandas as pd
from utils.data_loader import load_chunks
import config # ì„¤ì • íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¨ë‹¤. ì´ì œ í•˜ë“œì½”ë”©ì€ ê·¸ë§Œ.
import PyPDF2
from docx import Document
import io
from typing import Optional, Tuple
from utils.s3_store import S3Store

# â”€â”€ í™˜ê²½ ë° í´ë¼ì´ì–¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

app = FastAPI()

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
app.mount("/static", StaticFiles(directory="static"), name="static")

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€: ì´ì œ ë³´ì•ˆì„ ì¡°ê¸ˆ ë” ì‹ ê²½ ì“´ ì„¤ì •ìœ¼ë¡œ.
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.ALLOWED_ORIGINS, # config.pyì— ì •ì˜ëœ ì£¼ì†Œë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def log_chat(ts, user_input, role, temp, reply):
    os.makedirs(config.LOG_DIR, exist_ok=True)
    path = config.CHAT_LOG_PATH
    new  = not Path(path).exists()
    with open(path, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, ["timestamp", "user_input", "system_role", "temperature", "reply"])
        if new: w.writeheader()
        w.writerow({
            "timestamp": ts, "user_input": user_input,
            "system_role": role, "temperature": temp, "reply": reply
        })

def ensure_faiss_index():
    """index.faiss ê°€ ì—†ê±°ë‚˜ text íŒŒì¼ì´ ìƒˆë¡œ ìˆ˜ì •ëìœ¼ë©´ ìë™ ìƒì„±."""
    idx_path = Path(config.INDEX_PATH)
    txt_path = Path(config.TEXT_PATH)
    if not idx_path.exists() or (txt_path.exists() and txt_path.stat().st_mtime > idx_path.stat().st_mtime):
        print("ğŸ”„ FAISS ì¸ë±ìŠ¤ë¥¼ ë‹¤ì‹œ ë§Œë“œëŠ” ì¤‘... ì ì‹œ ê¸°ë‹¤ë ¤.")
        try:
            from experiments.generate_embedding import build_index
            chunks = load_chunks()
            build_index(chunks)
            print("âœ… index.faiss ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ Faiss ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

def embed_text(text: str):
    try:
        emb = client.embeddings.create(input=text, model=config.EMBED_MODEL).data[0].embedding
        return np.asarray(emb, dtype="float32").reshape(1, -1)
    except Exception as e:
        # OpenAI APIì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´, ì„œë²„ê°€ ì£½ëŠ” ëŒ€ì‹  í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë ¤ì¤€ë‹¤.
        raise HTTPException(status_code=500, detail=f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")

# ---------- íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_text_from_file(file: UploadFile) -> str:
    """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        content = file.file.read()
        
        if file.filename.endswith('.txt'):
            return content.decode('utf-8')
        
        elif file.filename.endswith('.pdf'):
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        
        elif file.filename.endswith('.docx'):
            doc = Document(io.BytesIO(content))
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        
        elif file.filename.endswith('.csv'):
            # CSVë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            csv_text = content.decode('utf-8')
            return csv_text
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file.filename}")
            
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

def chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    return text_splitter.split_text(text)

def rebuild_index(chunks: list) -> dict:
    """ìƒˆë¡œìš´ ì²­í¬ë“¤ë¡œ FAISS ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
    return rebuild_index_for_paths(chunks, config.INDEX_PATH, config.TEXT_PATH)

def rebuild_index_for_paths(chunks: list, index_path: str, text_path: str) -> dict:
    """ì„¸ì…˜ë³„ ê²½ë¡œë¥¼ ë°›ì•„ ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
    try:
        # ê¸°ì¡´ ì²­í¬ ë¡œë“œ
        existing_chunks = load_chunks(path=text_path)
        
        # ìƒˆ ì²­í¬ ì¶”ê°€
        all_chunks = existing_chunks + chunks
        
        # ì„ë² ë”© ìƒì„±
        print(f"ğŸ”„ {len(all_chunks)}ê°œ ì²­í¬ì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
        embeddings = []
        for i, chunk in enumerate(all_chunks):
            if i % 10 == 0:
                print(f"ì§„í–‰ë¥ : {i}/{len(all_chunks)}")
            emb = client.embeddings.create(input=chunk, model=config.EMBED_MODEL).data[0].embedding
            embeddings.append(emb)
        
        embeddings = np.array(embeddings, dtype='float32')
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)
        index.add(embeddings)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        Path(index_path).parent.mkdir(parents=True, exist_ok=True)
        faiss.write_index(index, index_path)
        
        # ìƒˆ ì²­í¬ë“¤ì„ í…ìŠ¤íŠ¸ íŒŒì¼ì— ì¶”ê°€
        Path(text_path).parent.mkdir(parents=True, exist_ok=True)
        with open(text_path, 'a', encoding='utf-8') as f:
            for chunk in chunks:
                f.write(chunk + '\n\n')
        
        return {
            "total_chunks": len(all_chunks),
            "new_chunks": len(chunks),
            "index_size_mb": os.path.getsize(index_path) / (1024 * 1024)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì˜¤ë¥˜: {str(e)}")

def get_s3_store() -> Optional[S3Store]:
    if getattr(config, "USE_S3", False) and config.S3_BUCKET:
        return S3Store(
            bucket=config.S3_BUCKET,
            region=config.S3_REGION,
            prefix=config.S3_PREFIX,
            aws_access_key_id=config.AWS_ACCESS_KEY_ID or None,
            aws_secret_access_key=config.AWS_SECRET_ACCESS_KEY or None,
            aws_session_token=config.AWS_SESSION_TOKEN or None,
        )
    return None

def get_paths_for_session(session_id: Optional[str]) -> Tuple[str, str]:
    if not session_id:
        return config.INDEX_PATH, config.TEXT_PATH
    base = Path("data/sessions") / session_id
    index_path = str(base / "index.faiss")
    text_path = str(base / "text_chunks.txt")
    return index_path, text_path

# ---------- API ë¼ìš°íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/")
async def read_root():
    """ë£¨íŠ¸ ê²½ë¡œì—ì„œ index.html íŒŒì¼ì„ ì„œë¹™í•œë‹¤."""
    try:
        with open("static/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        # í˜¹ì‹œë¼ë„ index.htmlì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì¹œì ˆí•œ ì•ˆë‚´.
        raise HTTPException(status_code=404, detail="index.html íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_session_id_from(req: Request, body: Optional[dict] = None) -> Optional[str]:
    try:
        sid = req.headers.get("X-Session-Id")
    except Exception:
        sid = None
    if not sid and isinstance(body, dict):
        sid = body.get("session_id")
    if sid:
        sid = str(sid).strip()
    return sid or None

def append_index_for_paths(chunks: list, index_path: str, text_path: str, session_id: Optional[str] = None) -> dict:
    """ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ìƒˆë¡œìš´ ì²­í¬ë§Œ ì„ë² ë”©í•˜ì—¬ ì¶”ê°€í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•œë‹¤."""
    try:
        Path(index_path).parent.mkdir(parents=True, exist_ok=True)
        Path(text_path).parent.mkdir(parents=True, exist_ok=True)

        s3 = get_s3_store()
        index = None
        current_total = 0
        if session_id and s3:
            # ì›ê²©ì—ì„œ ì½ê¸° ì‹œë„
            if s3.exists(session_id, "index.faiss"):
                index = s3.get_faiss(session_id, "index.faiss")
                current_total = index.ntotal
        else:
            index_exists = Path(index_path).exists()
            if index_exists:
                index = faiss.read_index(index_path)
                current_total = index.ntotal

        if not chunks:
            size_mb = 0.0
            if session_id and s3:
                # í¬ê¸° ì¡°íšŒëŠ” ìƒëµ
                size_mb = 0.0
            else:
                size_mb = (os.path.getsize(index_path) / (1024 * 1024)) if Path(index_path).exists() else 0.0
            return {"total_chunks": current_total, "new_chunks": 0, "index_size_mb": size_mb}

        # ìƒˆë¡œìš´ ì²­í¬ ì„ë² ë”©
        print(f"â• ìƒˆ ì²­í¬ {len(chunks)}ê°œ ì„ë² ë”© ì¶”ê°€ ì¤‘â€¦")
        new_embeds = []
        for i, chunk in enumerate(chunks, 1):
            if i % 10 == 0:
                print(f"ì§„í–‰ë¥ : {i}/{len(chunks)}")
            emb = client.embeddings.create(input=chunk, model=config.EMBED_MODEL).data[0].embedding
            new_embeds.append(emb)

        new_embeds = np.array(new_embeds, dtype='float32')

        # ì¸ë±ìŠ¤ì— ì¶”ê°€ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        if index is None:
            dim = new_embeds.shape[1]
            index = faiss.IndexFlatIP(dim)
            index.add(new_embeds)
        else:
            index.add(new_embeds)

        # ì €ì¥: S3 ìš°ì„ , ì—†ìœ¼ë©´ ë¡œì»¬
        if session_id and s3:
            s3.put_faiss(session_id, "index.faiss", index)
            # í…ìŠ¤íŠ¸ append
            s3.append_text(session_id, "text_chunks.txt", "".join([c + "\n\n" for c in chunks]))
            size_mb = 0.0
        else:
            faiss.write_index(index, index_path)
            with open(text_path, 'a', encoding='utf-8') as f:
                for chunk in chunks:
                    f.write(chunk + '\n\n')
            size_mb = os.path.getsize(index_path) / (1024 * 1024)

        total = current_total + len(chunks)
        return {"total_chunks": total, "new_chunks": len(chunks), "index_size_mb": size_mb}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¸ë±ìŠ¤ ì¶”ê°€ ì˜¤ë¥˜: {str(e)}")

@app.post("/search")
async def search(req: Request):
    """RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  GPT ë‹µë³€ê³¼ ì°¸ì¡° ë¬¸ì„œë¥¼ ë°˜í™˜í•œë‹¤."""
    try:
        body  = await req.json()
        q     = body.get("question", "")
        top_k = int(body.get("top_k", 3))
        temp  = float(body.get("temperature", 0.7))
        sys_p = body.get("system_prompt", "ë‹¤ìŒ ë¬¸ë‹¨ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
        session_id = get_session_id_from(req, body)
        index_path, text_path = get_paths_for_session(session_id)
        local_ctx = body.get("local_context")
 
         if not q:
             raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
 
         # ì„¸ì…˜ ê¸°ë°˜ ê²½ë¡œê°€ ì§€ì •ë˜ì—ˆì§€ë§Œ ì•„ì§ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°, ëª…í™•í•œ ì•ˆë‚´ ì œê³µ
         if session_id and (not Path(index_path).exists() or not Path(text_path).exists()):
             if not local_ctx:
                 raise HTTPException(status_code=404, detail="í˜„ì¬ ì„¸ì…˜ì— ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê±°ë‚˜ ì„¸ì…˜ì„ ì´ˆê¸°í™”í•˜ì„¸ìš”.")

         # 1. Retrieval or fallback to local context
         if local_ctx:
             top_chunks = [c for c in str(local_ctx).split("\n\n") if c.strip()][:top_k]
         else:
             vec = embed_text(q)
             s3 = get_s3_store()
             if session_id and s3:
                 # ì„¸ì…˜ + S3: ì›ê²©ì—ì„œ ë¡œë“œ
                 if not s3.exists(session_id, "index.faiss") or not s3.exists(session_id, "text_chunks.txt"):
                     raise HTTPException(status_code=404, detail="í˜„ì¬ ì„¸ì…˜ì— ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
                 try:
                     index = s3.get_faiss(session_id, "index.faiss")
                 except Exception as e:
                     raise HTTPException(status_code=500, detail=f"S3ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
                 try:
                     import re
                     content = s3.get_text(session_id, "text_chunks.txt")
                     parts = re.split(r"\n{2,}", content)
                     chunks = [c.strip() for c in parts if c.strip()]
                 except Exception as e:
                     raise HTTPException(status_code=500, detail=f"S3ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
             else:
                 try:
                     index = faiss.read_index(index_path)
                 except Exception as e:
                     raise HTTPException(status_code=404, detail=f"ì¸ë±ìŠ¤ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”. ({e})")
                 try:
                     chunks = load_chunks(path=text_path)
                 except FileNotFoundError:
                     raise HTTPException(status_code=404, detail="í…ìŠ¤íŠ¸ ì¡°ê° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
             if len(chunks) == 0 or index.ntotal == 0:
                 raise HTTPException(status_code=500, detail="ê²€ìƒ‰ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë§ë­‰ì¹˜ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
             k = min(top_k, index.ntotal, len(chunks))
             D, I = index.search(vec, k)
             valid_pairs = [(rank, idx, float(D[0][rank])) for rank, idx in enumerate(I[0]) if 0 <= idx < len(chunks)]
             top_chunks = [chunks[idx] for _, idx, _ in valid_pairs]

         # 2. Generation
         ctx = "\n\n".join(top_chunks)
         messages = [
             {"role": "system", "content": sys_p},
             {"role": "user", "content": f"{ctx}\n\nì§ˆë¬¸: {q}"}
         ]
         res = client.chat.completions.create(model=config.CHAT_MODEL, messages=messages, temperature=temp)
         ans = res.choices[0].message.content.strip()

         return {
             "question": q,
             "top_k": top_k,
             "temperature": temp,
             "system_prompt": sys_p,
             "session_id": session_id,
             "top_chunks": [
                 {"rank": r + 1, "text": chunks[idx], "distance": dist}
                 for r, (_, idx, dist) in enumerate(valid_pairs)
             ],
             "gpt_answer": ans
         }
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail="Faiss ì¸ë±ìŠ¤ ë˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        # ì´ì œ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ê°€ ë‚˜ë„ ì„œë²„ëŠ” ì£½ì§€ ì•ŠëŠ”ë‹¤.
        return JSONResponse(status_code=500, content={"message": f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}"})

@app.post("/upload")
async def upload_files(
    files: list[UploadFile] = File(...),
    chunk_size: int = Form(512),
    chunk_overlap: int = Form(100),
    session_id: Optional[str] = Form(None)
):
    """íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  RAG ì‹œìŠ¤í…œì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        start_time = time.time()
        index_path, text_path = get_paths_for_session(session_id)
        
        if not files:
            raise HTTPException(status_code=400, detail="ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        all_chunks = []
        processed_files = 0
        
        for file in files:
            if not file.filename:
                continue
                
            # íŒŒì¼ í¬ê¸° ì œí•œ (10MB) - ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°
            try:
                current_pos = file.file.tell()
                file.file.seek(0, os.SEEK_END)
                file_size = file.file.tell()
                file.file.seek(current_pos)
            except Exception:
                file_size = None
            if file_size is not None and file_size > 10 * 1024 * 1024:
                raise HTTPException(status_code=400, detail=f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤: {file.filename}")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = extract_text_from_file(file)
            
            # ì²­í‚¹
            chunks = chunk_text(text, chunk_size, chunk_overlap)
            all_chunks.extend(chunks)
            processed_files += 1
            
            print(f"âœ… {file.filename} ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        if not all_chunks:
            raise HTTPException(status_code=400, detail="ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¸ë±ìŠ¤ì— ìƒˆ ì²­í¬ë§Œ ì¶”ê°€ (ì¬êµ¬ì„± ëŒ€ì‹ )
        index_info = append_index_for_paths(all_chunks, index_path, text_path, session_id=session_id)
        
        processing_time = time.time() - start_time
        
        return {
            "files_processed": processed_files,
            "chunks_created": len(all_chunks),
            "processing_time": round(processing_time, 2),
            "index_size": round(index_info["index_size_mb"], 2),
            "total_chunks": index_info["total_chunks"],
            "session_id": session_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/add-text")
async def add_text_document(
    title: str = Form(...),
    content: str = Form(...),
    chunk_size: int = Form(512),
    chunk_overlap: int = Form(100),
    session_id: Optional[str] = Form(None)
):
    """í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        start_time = time.time()
        index_path, text_path = get_paths_for_session(session_id)
        
        # ì…ë ¥ ê²€ì¦
        if not title or not title.strip():
            raise HTTPException(status_code=400, detail="ë¬¸ì„œ ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        if not content or not content.strip():
            raise HTTPException(status_code=400, detail="ë¬¸ì„œ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì œëª© ê¸¸ì´ ì œí•œ
        if len(title.strip()) > 200:
            raise HTTPException(status_code=400, detail="ë¬¸ì„œ ì œëª©ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 200ì ì´í•˜ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (1MB)
        content_bytes = content.encode('utf-8')
        if len(content_bytes) > 1024 * 1024:
            raise HTTPException(status_code=400, detail="í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 1MB ì´í•˜ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì²­í‚¹ íŒŒë¼ë¯¸í„° ê²€ì¦
        if chunk_size < 100 or chunk_size > 2000:
            raise HTTPException(status_code=400, detail="ì²­í¬ í¬ê¸°ëŠ” 100-2000 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        if chunk_overlap < 0 or chunk_overlap >= chunk_size:
            raise HTTPException(status_code=400, detail="ì²­í¬ ê²¹ì¹¨ì€ 0 ì´ìƒì´ê³  ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
        
        # ì²­í‚¹
        chunks = chunk_text(content, chunk_size, chunk_overlap)
        
        if not chunks:
            raise HTTPException(status_code=400, detail="ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¸ë±ìŠ¤ì— ìƒˆ ì²­í¬ë§Œ ì¶”ê°€ (ì¬êµ¬ì„± ëŒ€ì‹ )
        index_info = append_index_for_paths(chunks, index_path, text_path, session_id=session_id)
        
        processing_time = time.time() - start_time
        
        # ë¡œê¹… ê°œì„ 
        print(f"âœ… í…ìŠ¤íŠ¸ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   - ì œëª©: {title}")
        print(f"   - ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")
        print(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"   - ì¸ë±ìŠ¤ í¬ê¸°: {index_info['index_size_mb']:.2f}MB")
        
        return {
            "title": title,
            "chunks_created": len(chunks),
            "processing_time": round(processing_time, 2),
            "index_size": round(index_info["index_size_mb"], 2),
            "total_chunks": index_info["total_chunks"],
            "content_size_bytes": len(content_bytes),
            "session_id": session_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/results")
async def get_results():
    """ì‹¤í—˜ ê²°ê³¼ CSV íŒŒì¼ì„ ì½ì–´ JSONìœ¼ë¡œ ë°˜í™˜í•œë‹¤."""
    try:
        results_dir = Path("experiments/results")
        csv_files = sorted(results_dir.glob("grid_search_*.csv"), key=os.path.getmtime, reverse=True)
        if not csv_files:
            raise FileNotFoundError

        latest_csv = csv_files[0]
        df = pd.read_csv(latest_csv)
        
        if 'cost_cents' in df.columns:
            df['cost_dollars'] = (df['cost_cents']).round(4)

        top_10 = df.nlargest(10, ['recall@5', 'f1']).to_dict(orient='records')

        summary = {
            "total_experiments": len(df),
            "best_recall": df['recall@5'].max(),
            "best_f1": df['f1'].max(),
            "avg_latency_ms": df['latency_ms'].mean(),
            "avg_cost": df['cost_dollars'].mean() if 'cost_dollars' in df.columns else df['cost_cents'].mean(),
            "perfect_scores": len(df[(df['recall@5'] == 1.0) & (df['f1'] == 1.0)])
        }
        
        # ë¸Œë¼ìš°ì €ê°€ ì‘ë‹µì„ ìºì‹±í•˜ì§€ ì•Šë„ë¡ í—¤ë” ì¶”ê°€
        headers = {"Cache-Control": "no-cache, no-store, must-revalidate"}
        return JSONResponse(content={"summary": summary, "top_results": top_10}, headers=headers)

    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except KeyError as e:
        raise HTTPException(status_code=500, detail=f"ê²°ê³¼ íŒŒì¼ì—ì„œ í•„ìš”í•œ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ---------- ì•± ì‹œì‘ ì‹œ ì¸ë±ìŠ¤ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ensure_faiss_index()

# ---------- ì„œë²„ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
